{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4086,"status":"ok","timestamp":1720432963998,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"},"user_tz":-180},"id":"sChuxDxE2SVY","outputId":"91c06030-835e-4df8-e324-acd242fc4c2c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'heart_beat'...\n","remote: Enumerating objects: 128, done.\u001b[K\n","remote: Counting objects: 100% (128/128), done.\u001b[K\n","remote: Compressing objects: 100% (90/90), done.\u001b[K\n","remote: Total 128 (delta 42), reused 112 (delta 33), pack-reused 0\u001b[K\n","Receiving objects: 100% (128/128), 25.26 MiB | 14.78 MiB/s, done.\n","Resolving deltas: 100% (42/42), done.\n","/content/heart_beat\n"]}],"source":["!git clone https://github.com/elkhayyat17/heart_beat\n","%cd heart_beat"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17736,"status":"ok","timestamp":1720432983887,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"},"user_tz":-180},"id":"MM8u_t4Q2exi","outputId":"952aeaa4-8e79-4801-f157-2478113e1afa"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q -r requirements.txt\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30390,"status":"ok","timestamp":1720433014273,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"},"user_tz":-180},"id":"KN8CQF73Nz7G","outputId":"f904b661-ff03-44dc-8e82-e7277d9b0b2e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu121\n","Collecting llama-cpp-python\n","  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.81-cu121/llama_cpp_python-0.2.81-cp310-cp310-linux_x86_64.whl (282.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.8/282.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions\u003e=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\n","Requirement already satisfied: numpy\u003e=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n","Collecting diskcache\u003e=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: jinja2\u003e=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2\u003e=2.11.3-\u003ellama-cpp-python) (2.1.5)\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.81\n"]}],"source":["!pip install llama-cpp-python \\\n","  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29096,"status":"ok","timestamp":1720433043341,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"},"user_tz":-180},"id":"heWqdTV9N1fV","outputId":"890b8db1-987a-4399-e207-ba7a63219596"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-07-08 10:03:35--  https://huggingface.co/Elkhayyat17/llama2-Med-gguf/resolve/main/ggml-model-Q5_K_M.gguf\n","Resolving huggingface.co (huggingface.co)... 13.33.30.76, 13.33.30.114, 13.33.30.49, ...\n","Connecting to huggingface.co (huggingface.co)|13.33.30.76|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://cdn-lfs-us-1.huggingface.co/repos/0c/33/0c339bc47ef4bb2188a662e332c337cfed1ab1c16cd0befb7d0c30c2a12fc011/00f7eadaddd044c3530ec0711445a18abdfbede4b877c3301a622f589cadba7b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ggml-model-Q5_K_M.gguf%3B+filename%3D%22ggml-model-Q5_K_M.gguf%22%3B\u0026Expires=1720692215\u0026Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDY5MjIxNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBjLzMzLzBjMzM5YmM0N2VmNGJiMjE4OGE2NjJlMzMyYzMzN2NmZWQxYWIxYzE2Y2QwYmVmYjdkMGMzMGMyYTEyZmMwMTEvMDBmN2VhZGFkZGQwNDRjMzUzMGVjMDcxMTQ0NWExOGFiZGZiZWRlNGI4NzdjMzMwMWE2MjJmNTg5Y2FkYmE3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19\u0026Signature=F4NDmL4bHZDe6xJzKMhUrmVfqk4eZG-jIKLD0iHqahLnJS4wBNwMSUXfaXZZV%7EcedwUIGHH21dMRlDGtJLRjnEhxuVAVD8eZpRe0ZkyJ%7E5R%7E1xWFicA2iJ%7Ey5hZSRxbCyD7moT73k5pjwhzWsAdckPiXV98sgII7H%7EF%7EIHQpq-c6uRWf2rpH4o2DpdFRZFq0VoKEo0S36vB3kfLGC60650RNVXquhx5by%7EEnAcBopk5vfSky8rCLYQu2TzgBOXp6b8exuRNHCJiH7eIWRzuzE2AMZRO1ZIT3AmNwSVH7CUmeQ%7EUndsIf31FjMLdyP4dyNzCTmCl%7EgTlrnHzE%7EPZBUQ__\u0026Key-Pair-Id=K24J24Z295AEI9 [following]\n","--2024-07-08 10:03:35--  https://cdn-lfs-us-1.huggingface.co/repos/0c/33/0c339bc47ef4bb2188a662e332c337cfed1ab1c16cd0befb7d0c30c2a12fc011/00f7eadaddd044c3530ec0711445a18abdfbede4b877c3301a622f589cadba7b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27ggml-model-Q5_K_M.gguf%3B+filename%3D%22ggml-model-Q5_K_M.gguf%22%3B\u0026Expires=1720692215\u0026Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMDY5MjIxNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzBjLzMzLzBjMzM5YmM0N2VmNGJiMjE4OGE2NjJlMzMyYzMzN2NmZWQxYWIxYzE2Y2QwYmVmYjdkMGMzMGMyYTEyZmMwMTEvMDBmN2VhZGFkZGQwNDRjMzUzMGVjMDcxMTQ0NWExOGFiZGZiZWRlNGI4NzdjMzMwMWE2MjJmNTg5Y2FkYmE3Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19\u0026Signature=F4NDmL4bHZDe6xJzKMhUrmVfqk4eZG-jIKLD0iHqahLnJS4wBNwMSUXfaXZZV%7EcedwUIGHH21dMRlDGtJLRjnEhxuVAVD8eZpRe0ZkyJ%7E5R%7E1xWFicA2iJ%7Ey5hZSRxbCyD7moT73k5pjwhzWsAdckPiXV98sgII7H%7EF%7EIHQpq-c6uRWf2rpH4o2DpdFRZFq0VoKEo0S36vB3kfLGC60650RNVXquhx5by%7EEnAcBopk5vfSky8rCLYQu2TzgBOXp6b8exuRNHCJiH7eIWRzuzE2AMZRO1ZIT3AmNwSVH7CUmeQ%7EUndsIf31FjMLdyP4dyNzCTmCl%7EgTlrnHzE%7EPZBUQ__\u0026Key-Pair-Id=K24J24Z295AEI9\n","Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 3.165.102.25, 3.165.102.80, 3.165.102.112, ...\n","Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|3.165.102.25|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4783157824 (4.5G) [binary/octet-stream]\n","Saving to: ‘ggml-model-Q5_K_M.gguf’\n","\n","ggml-model-Q5_K_M.g 100%[===================\u003e]   4.45G   175MB/s    in 29s     \n","\n","2024-07-08 10:04:04 (159 MB/s) - ‘ggml-model-Q5_K_M.gguf’ saved [4783157824/4783157824]\n","\n"]}],"source":["!wget https://huggingface.co/Elkhayyat17/llama2-Med-gguf/resolve/main/ggml-model-Q5_K_M.gguf\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10099,"status":"ok","timestamp":1720433053415,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"},"user_tz":-180},"id":"_k12hRCj2g4f","outputId":"708b39f6-6d50-4bdc-ed15-9fced796d174"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.30.1)\n","Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.111.0)\n","Collecting pyngrok\n","  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n","Requirement already satisfied: click\u003e=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n","Requirement already satisfied: h11\u003e=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n","Requirement already satisfied: typing-extensions\u003e=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.12.2)\n","Requirement already satisfied: starlette\u003c0.38.0,\u003e=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.37.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,\u003c3.0.0,\u003e=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.8.0)\n","Requirement already satisfied: fastapi-cli\u003e=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.0.4)\n","Requirement already satisfied: httpx\u003e=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.27.0)\n","Requirement already satisfied: jinja2\u003e=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.1.4)\n","Requirement already satisfied: python-multipart\u003e=0.0.7 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.0.9)\n","Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,\u003e=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (5.10.0)\n","Requirement already satisfied: orjson\u003e=3.2.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.10.6)\n","Requirement already satisfied: email_validator\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.2.0)\n","Requirement already satisfied: PyYAML\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n","Requirement already satisfied: dnspython\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator\u003e=2.0.0-\u003efastapi) (2.6.1)\n","Requirement already satisfied: idna\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator\u003e=2.0.0-\u003efastapi) (3.7)\n","Requirement already satisfied: typer\u003e=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli\u003e=0.0.2-\u003efastapi) (0.12.3)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx\u003e=0.23.0-\u003efastapi) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx\u003e=0.23.0-\u003efastapi) (2024.6.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx\u003e=0.23.0-\u003efastapi) (1.0.5)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx\u003e=0.23.0-\u003efastapi) (1.3.1)\n","Requirement already satisfied: MarkupSafe\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2\u003e=2.11.2-\u003efastapi) (2.1.5)\n","Requirement already satisfied: annotated-types\u003e=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,\u003c3.0.0,\u003e=1.7.4-\u003efastapi) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,\u003c3.0.0,\u003e=1.7.4-\u003efastapi) (2.20.0)\n","Requirement already satisfied: httptools\u003e=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.6.1)\n","Requirement already satisfied: python-dotenv\u003e=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (1.0.1)\n","Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,\u003e=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.19.0)\n","Requirement already satisfied: watchfiles\u003e=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.22.0)\n","Requirement already satisfied: websockets\u003e=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (12.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio-\u003ehttpx\u003e=0.23.0-\u003efastapi) (1.2.1)\n","Requirement already satisfied: shellingham\u003e=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003e=0.12.3-\u003efastapi-cli\u003e=0.0.2-\u003efastapi) (1.5.4)\n","Requirement already satisfied: rich\u003e=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer\u003e=0.12.3-\u003efastapi-cli\u003e=0.0.2-\u003efastapi) (13.7.1)\n","Requirement already satisfied: markdown-it-py\u003e=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003e=0.12.3-\u003efastapi-cli\u003e=0.0.2-\u003efastapi) (3.0.0)\n","Requirement already satisfied: pygments\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich\u003e=10.11.0-\u003etyper\u003e=0.12.3-\u003efastapi-cli\u003e=0.0.2-\u003efastapi) (2.16.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py\u003e=2.2.0-\u003erich\u003e=10.11.0-\u003etyper\u003e=0.12.3-\u003efastapi-cli\u003e=0.0.2-\u003efastapi) (0.1.2)\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-7.1.6\n"]}],"source":["!pip install uvicorn fastapi pyngrok\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36426,"status":"ok","timestamp":1720433092019,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"},"user_tz":-180},"id":"mai15Dc0N71F","outputId":"a9bf7f6b-277a-4b0c-907f-79de41ccc541"},"outputs":[{"name":"stderr","output_type":"stream","text":["llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ggml-model-Q5_K_M.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 17\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"\u003cunk\u003e\", \"\u003cs\u003e\", \"\u003c/s\u003e\", \"\u003c0x00\u003e\", \"\u003c...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\n","llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q5_K:  193 tensors\n","llama_model_loader: - type q6_K:   33 tensors\n","llm_load_vocab: special tokens cache size = 259\n","llm_load_vocab: token to piece cache size = 0.1684 MB\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32000\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: n_ctx_train      = 4096\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 32\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_swa            = 0\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 4096\n","llm_load_print_meta: n_embd_v_gqa     = 4096\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 11008\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 10000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_ctx_orig_yarn  = 4096\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q5_K - Medium\n","llm_load_print_meta: model params     = 6.74 B\n","llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n","llm_load_print_meta: general.name     = LLaMA v2\n","llm_load_print_meta: BOS token        = 1 '\u003cs\u003e'\n","llm_load_print_meta: EOS token        = 2 '\u003c/s\u003e'\n","llm_load_print_meta: UNK token        = 0 '\u003cunk\u003e'\n","llm_load_print_meta: LF token         = 13 '\u003c0x0A\u003e'\n","llm_load_print_meta: max token length = 48\n","ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n","ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n","ggml_cuda_init: found 1 CUDA devices:\n","  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n","llm_load_tensors: ggml ctx size =    0.27 MiB\n","llm_load_tensors: offloading 32 repeating layers to GPU\n","llm_load_tensors: offloading non-repeating layers to GPU\n","llm_load_tensors: offloaded 33/33 layers to GPU\n","llm_load_tensors:        CPU buffer size =    85.94 MiB\n","llm_load_tensors:      CUDA0 buffer size =  4474.94 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 2048\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 10000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n","llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n","llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n","llama_new_context_with_model:      CUDA0 compute buffer size =   164.00 MiB\n","llama_new_context_with_model:  CUDA_Host compute buffer size =    12.01 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 2\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n","Model metadata: {'tokenizer.chat_template': \"{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '\u003c\u003cSYS\u003e\u003e\\\\n' + system_message + '\\\\n\u003c\u003c/SYS\u003e\u003e\\\\n\\\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\", 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '17'}\n","Available chat formats from metadata: chat_template.default\n"]}],"source":["import io\n","from fastapi import UploadFile, HTTPException, FastAPI\n","from fastapi.middleware.cors import CORSMiddleware\n","from pydantic import BaseModel\n","import torch\n","import librosa\n","from model import predict ,Diagnose\n","import json\n","from PIL import Image\n","\n","\n","app = FastAPI()\n","from llama_cpp import Llama\n","llm = Llama(\n","      model_path=\"ggml-model-Q5_K_M.gguf\",\n","       n_gpu_layers=-1,n_ctx=2048,chat_format=\"llama-2\"\n",")\n","app.add_middleware(\n","    CORSMiddleware,\n","    allow_origins=[\"*\"],\n","    allow_credentials=True,\n","    allow_methods=[\"*\"],\n","    allow_headers=[\"*\"],\n",")\n","\n","def ask(quest):\n","    out=llm.create_chat_completion(\n","      messages = [{\"role\": \"system\", \"content\": '''You are Doctor Sakina,  a virtual AI doctor known for your friendly and approachable demeanor,\n","  combined with a deep expertise in the medical field. You're here to provide professional, empathetic, and knowledgeable advice on health-related inquiries.\n","  You'll also provide differential diagnosis. If you're unsure about any information, Don't share false information.'''},\n","  {\"role\": \"user\", \"content\": f\" Symptoms:{quest}\"} ],\n","\n","    temperature=0.001,\n",")\n","    return str(out['choices'][0]['message']['content'])\n","\n","\n","\n","\n","class ChatBotRequest(BaseModel):\n","    message: str\n","\n","@app.post(\"/chatBot\")\n","async def detect(request: ChatBotRequest):\n","    return { \"response\":  ask(request.message) }\n","@app.post(\"/heartChecking\")\n","async def detect(voice: UploadFile):\n","    if voice.filename.split(\".\")[-1] not in (\"wav\", \"mp3\"):\n","        raise HTTPException(\n","            status_code=415, detail=\"Not a Voice\"\n","        )\n","\n","    audio_bytes = voice.file.read()\n","    audio, sample_rate = librosa.load(io.BytesIO(audio_bytes), sr=None)\n","    prediction = await predict(audio, sample_rate)\n","\n","    response = json.load(open(\"responses/heart.json\"))\n","\n","    match prediction:\n","        case \"murmur\":\n","            return response[\"murmur\"]\n","        case \"normal\":\n","            return response[\"normal\"]\n","        case \"artifact\":\n","            return response[\"artifact\"]\n","        case \"extrastole\":\n","            return response[\"extrasystole\"]\n","        case \"extrahls\":\n","            return response[\"extraHeartSound\"]\n","\n","\n","@app.post(\"/skinChecking\")\n","async def detect(image: UploadFile):\n","    if image.filename.split(\".\")[-1] not in ('jpg', 'jpeg', 'png', 'gif', 'svg', 'bmp', 'webp', 'tiff'):\n","        raise HTTPException(\n","            status_code=415, detail=\"Not an image\"\n","        )\n","    # Load the image\n","\n","    prediction = await  Diagnose(image.file.read())\n","\n","    response = json.load(open(\"responses/skin.json\"))\n","\n","    match prediction:\n","            case \"akiec\":\n","                return response[\"akiec\"]\n","            case \"bcc\":\n","                return response[\"bcc\"]\n","            case \"bkl\":\n","                return response[\"bkl\"]\n","            case \"df\":\n","                return response[\"df\"]\n","            case \"mel\":\n","                return response[\"mel\"]\n","            case \"nv\":\n","                return response[\"nv\"]\n","            case \"vasc\":\n","                return response[\"vasc\"]\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3273,"status":"ok","timestamp":1720433095287,"user":{"displayName":"Ahmed Elkhayyat","userId":"12405653964529109374"},"user_tz":-180},"id":"h42Kh368N9t9","outputId":"7f477626-b2d2-42c4-8877-2b507bd6b01d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}],"source":["!ngrok config add-authtoken 2dSYzvI6BTpQoVRCtsb09nxiOEm_5NjwRJ4K2EQxqNykPnr19"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"-mAYh65FN9vu"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:     Started server process [238]\n","INFO:     Waiting for application startup.\n","INFO:     Application startup complete.\n","INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"]},{"name":"stdout","output_type":"stream","text":["Public URL: https://6945-34-142-133-164.ngrok-free.app\n","INFO:     197.35.13.86:0 - \"GET / HTTP/1.1\" 404 Not Found\n","INFO:     197.35.13.86:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n","INFO:     197.35.13.86:0 - \"GET /docs HTTP/1.1\" 200 OK\n","INFO:     197.35.13.86:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n","1/1 [==============================] - 2s 2s/step\n","INFO:     197.35.13.86:0 - \"POST /skinChecking HTTP/1.1\" 200 OK\n","INFO:     197.35.13.86:0 - \"GET / HTTP/1.1\" 404 Not Found\n","INFO:     197.35.13.86:0 - \"GET /docs HTTP/1.1\" 200 OK\n","1/1 [==============================] - 0s 437ms/step\n","INFO:     197.35.13.86:0 - \"POST /heartChecking HTTP/1.1\" 200 OK\n","INFO:     149.154.161.253:0 - \"GET /docs HTTP/1.1\" 200 OK\n","INFO:     197.35.61.27:0 - \"GET /docs HTTP/1.1\" 200 OK\n","INFO:     197.35.61.27:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n","INFO:     197.35.61.27:0 - \"OPTIONS /heartChecking HTTP/1.1\" 200 OK\n","1/1 [==============================] - 0s 26ms/step\n","INFO:     197.35.61.27:0 - \"POST /heartChecking HTTP/1.1\" 200 OK\n","INFO:     197.35.61.27:0 - \"OPTIONS /heartChecking HTTP/1.1\" 200 OK\n","1/1 [==============================] - 0s 29ms/step\n","INFO:     197.35.61.27:0 - \"POST /heartChecking HTTP/1.1\" 200 OK\n"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =     473.72 ms\n","llama_print_timings:      sample time =      29.36 ms /    53 runs   (    0.55 ms per token,  1805.24 tokens per second)\n","llama_print_timings: prompt eval time =     473.50 ms /   113 tokens (    4.19 ms per token,   238.65 tokens per second)\n","llama_print_timings:        eval time =    1549.72 ms /    52 runs   (   29.80 ms per token,    33.55 tokens per second)\n","llama_print_timings:       total time =    2094.23 ms /   165 tokens\n"]},{"name":"stdout","output_type":"stream","text":["INFO:     197.35.61.27:0 - \"POST /chatBot HTTP/1.1\" 200 OK\n","INFO:     197.35.13.86:0 - \"OPTIONS /skinChecking HTTP/1.1\" 200 OK\n","1/1 [==============================] - 0s 117ms/step\n","INFO:     197.35.13.86:0 - \"POST /skinChecking HTTP/1.1\" 200 OK\n","INFO:     197.35.61.27:0 - \"OPTIONS /skinChecking HTTP/1.1\" 200 OK\n","1/1 [==============================] - 0s 109ms/step\n","INFO:     197.35.61.27:0 - \"POST /skinChecking HTTP/1.1\" 200 OK\n"]},{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =     473.72 ms\n","llama_print_timings:      sample time =     128.86 ms /   228 runs   (    0.57 ms per token,  1769.42 tokens per second)\n","llama_print_timings: prompt eval time =      96.80 ms /    14 tokens (    6.91 ms per token,   144.63 tokens per second)\n","llama_print_timings:        eval time =    7285.37 ms /   227 runs   (   32.09 ms per token,    31.16 tokens per second)\n","llama_print_timings:       total time =    7687.63 ms /   241 tokens\n"]},{"name":"stdout","output_type":"stream","text":["INFO:     197.35.61.27:0 - \"POST /chatBot HTTP/1.1\" 200 OK\n"]},{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =     473.72 ms\n","llama_print_timings:      sample time =      26.23 ms /    46 runs   (    0.57 ms per token,  1753.65 tokens per second)\n","llama_print_timings: prompt eval time =      69.03 ms /     9 tokens (    7.67 ms per token,   130.38 tokens per second)\n","llama_print_timings:        eval time =    1435.75 ms /    45 runs   (   31.91 ms per token,    31.34 tokens per second)\n","llama_print_timings:       total time =    1558.31 ms /    54 tokens\n"]},{"name":"stdout","output_type":"stream","text":["INFO:     197.35.61.27:0 - \"POST /chatBot HTTP/1.1\" 200 OK\n"]},{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =     473.72 ms\n","llama_print_timings:      sample time =     122.18 ms /   212 runs   (    0.58 ms per token,  1735.12 tokens per second)\n","llama_print_timings: prompt eval time =     110.34 ms /    42 tokens (    2.63 ms per token,   380.62 tokens per second)\n","llama_print_timings:        eval time =    6910.56 ms /   211 runs   (   32.75 ms per token,    30.53 tokens per second)\n","llama_print_timings:       total time =    7294.91 ms /   253 tokens\n"]},{"name":"stdout","output_type":"stream","text":["INFO:     197.35.61.27:0 - \"POST /chatBot HTTP/1.1\" 200 OK\n","INFO:     197.35.61.27:0 - \"OPTIONS /heartChecking HTTP/1.1\" 200 OK\n","1/1 [==============================] - 0s 25ms/step\n","INFO:     197.35.61.27:0 - \"POST /heartChecking HTTP/1.1\" 200 OK\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 8 calls to \u003cfunction Model.make_predict_function.\u003clocals\u003e.predict_function at 0x7b491995ad40\u003e triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 121ms/step\n","INFO:     197.35.61.27:0 - \"POST /skinChecking HTTP/1.1\" 200 OK\n"]},{"name":"stderr","output_type":"stream","text":["Llama.generate: prefix-match hit\n","\n","llama_print_timings:        load time =     473.72 ms\n","llama_print_timings:      sample time =     162.38 ms /   228 runs   (    0.71 ms per token,  1404.09 tokens per second)\n","llama_print_timings: prompt eval time =      72.19 ms /    14 tokens (    5.16 ms per token,   193.94 tokens per second)\n","llama_print_timings:        eval time =    7056.14 ms /   227 runs   (   31.08 ms per token,    32.17 tokens per second)\n","llama_print_timings:       total time =    8020.60 ms /   241 tokens\n"]},{"name":"stdout","output_type":"stream","text":["INFO:     197.35.61.27:0 - \"POST /chatBot HTTP/1.1\" 200 OK\n"]}],"source":["from pyngrok import ngrok\n","import nest_asyncio\n","import uvicorn\n","ngrok_tunnel = ngrok.connect(8000)\n","print('Public URL:', ngrok_tunnel.public_url)\n","nest_asyncio.apply()\n","uvicorn.run(app, port=8000)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOLJPErsXfUmCdDYErvLbFX","gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}